{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72b7383c-db62-4194-a23f-bb6b0af86878",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assumtions Being Made :\n",
    "# All Data is coming fom RDS and the staging is from S3\n",
    "# All staging tables are the same as RDS with a staging timestamp added\n",
    "\n",
    "# ** Source for OLAP is S3 and Target is Hive Tables\n",
    "# ** All tables in Source and Target have the same number of columns when negating record_active_flag , start_date and end_date or evqivalent and the names need to be exactly same\n",
    "# ** Stage Tables need to be joined before feeding it to any framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fd6ce2f-8e9f-4c06-9e9e-418efb1fbaf0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": ""
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": {
        "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
        "text/plain": ""
       },
       "datasetInfos": [],
       "executionCount": null,
       "metadata": {
        "kernelSessionId": "b309a47f-7ed068713f098bac49ede0cd"
       },
       "removedWidgets": [],
       "type": "mimeBundle"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import urllib\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.window import Window\n",
    "import datetime\n",
    "\n",
    "MOUNT_NAME = \"/mnt/election-project-stage\"\n",
    "op_MOUNT_NAME = \"/mnt/election-project-olap\"\n",
    "date_today = str(datetime.datetime.now()).split()[0]\n",
    "try:\n",
    "    dbutils.fs.ls(MOUNT_NAME)\n",
    "except:\n",
    "    file_type = \"csv\"\n",
    "    first_row_is_header = \"true\"\n",
    "    delimiter = \",\"\n",
    "    aws_keys_df = (\n",
    "        spark.read.format(file_type)\n",
    "        .option(\"header\", first_row_is_header)\n",
    "        .option(\"sep\", delimiter)\n",
    "        .load(\"dbfs:/FileStore/yahoo_rama_accessKeys.csv\")\n",
    "    )\n",
    "    ACCESS_KEY = aws_keys_df.select(\"Access key ID\").collect()[0][\"Access key ID\"]\n",
    "    SECRET_KEY = aws_keys_df.select(\"Secret access key\").collect()[0][\n",
    "        \"Secret access key\"\n",
    "    ]\n",
    "    ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")\n",
    "    AWS_S3_BUCKET = \"election-project-stage\"\n",
    "    SOURCE_URL = \"s3n://{0}:{1}@{2}\".format(\n",
    "        ACCESS_KEY, ENCODED_SECRET_KEY, AWS_S3_BUCKET\n",
    "    )\n",
    "    dbutils.fs.mount(SOURCE_URL, MOUNT_NAME)\n",
    "\n",
    "try:\n",
    "    dbutils.fs.ls(op_MOUNT_NAME)\n",
    "except:\n",
    "    file_type = \"csv\"\n",
    "    first_row_is_header = \"true\"\n",
    "    delimiter = \",\"\n",
    "    aws_keys_df = (\n",
    "        spark.read.format(file_type)\n",
    "        .option(\"header\", first_row_is_header)\n",
    "        .option(\"sep\", delimiter)\n",
    "        .load(\"dbfs:/FileStore/yahoo_rama_accessKeys.csv\")\n",
    "    )\n",
    "    ACCESS_KEY = aws_keys_df.select(\"Access key ID\").collect()[0][\"Access key ID\"]\n",
    "    SECRET_KEY = aws_keys_df.select(\"Secret access key\").collect()[0][\n",
    "        \"Secret access key\"\n",
    "    ]\n",
    "    ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")\n",
    "    op_AWS_S3_BUCKET = \"election-project-olap\"\n",
    "    op_SOURCE_URL = \"s3n://{0}:{1}@{2}\".format(\n",
    "        ACCESS_KEY, ENCODED_SECRET_KEY, op_AWS_S3_BUCKET\n",
    "    )\n",
    "    dbutils.fs.mount(op_SOURCE_URL, op_MOUNT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c47f407-50a6-45f5-82bf-9269e47daedc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_data(input_, scd_type=2):\n",
    "    \"\"\"Gets Data From S3 as well as RDS Tables based on the Table name and SCD type in the input\"\"\"\n",
    "    global table\n",
    "    global df\n",
    "    global df_dim\n",
    "    global table_old\n",
    "    table = \"_\".join(input_.lower().split())\n",
    "    if (\n",
    "        table == \"election_result_fact\"\n",
    "        or table.lower()[-4:] == \"fact\"\n",
    "        or table.lower()[-6:] == \"result\"\n",
    "        or table.lower()[:8] == \"election\"\n",
    "    ):\n",
    "        table = \"election_result\"\n",
    "        df = (\n",
    "            spark.read.format(\"parquet\")\n",
    "            .option(\"inferSchema\", \"true\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"sep\", \",\")\n",
    "            .load(\n",
    "                f\"{MOUNT_NAME}/counting_stage/as_on_dt={date_today}/counting_stage_parquet/\"\n",
    "            )\n",
    "        )\n",
    "        df_dim = (\n",
    "            spark.read.format(\"parquet\")\n",
    "            .option(\"inferSchema\", \"true\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"sep\", \",\")\n",
    "            .load(\n",
    "                f\"{op_MOUNT_NAME}/election-project-olap/election_result_fact_olap\"\n",
    "            )\n",
    "        )\n",
    "        dim_columns = df_dim.columns\n",
    "    else:\n",
    "        df = (\n",
    "            spark.read.format(\"parquet\")\n",
    "            .option(\"inferSchema\", \"true\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"sep\", \",\")\n",
    "            .load(\n",
    "                f\"/mnt/election-project-stage/{table}_stage/as_on_dt={date_today}/{table}_stage_parquet/\"\n",
    "            )\n",
    "        )\n",
    "        if scd_type == 1:\n",
    "            df_dim = spark.read.format(\"delta\").load(\n",
    "                f\"dbfs:/user/hive/warehouse/election_olap_scd_type_one.db/{table}_olap_scd_1\"\n",
    "            )\n",
    "        elif scd_type == 2:\n",
    "            df_dim = (\n",
    "            spark.read.format(\"parquet\")\n",
    "            .option(\"inferSchema\", \"true\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"sep\", \",\")\n",
    "            .load(\n",
    "                f\"{op_MOUNT_NAME}/election-project-olap/{table}_olap/\"\n",
    "            )\n",
    "        )\n",
    "#             df_dim = spark.read.format(\"delta\").load(\n",
    "#                 f\"dbfs:/user/hive/warehouse/election_olap.db/{table}_olap\"\n",
    "#             )\n",
    "        elif scd_type == 3:\n",
    "            df_dim = spark.read.format(\"delta\").load(\n",
    "                f\"dbfs:/user/hive/warehouse/election_olap_scd_type_three.db/{table}_olap_scd_3\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"Wrong SCD Type entered\")\n",
    "            sys.exit()\n",
    "        dim_columns = df_dim.columns\n",
    "        for value in dim_columns:\n",
    "            df_dim = df_dim.withColumnRenamed(value, \"dim_\" + value.lower())\n",
    "\n",
    "        if table == \"candidates\":\n",
    "            table_old = table[:]\n",
    "            table = table[:-1]\n",
    "        else:\n",
    "            table = table\n",
    "            table_old = table\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aafdf00a-8bb0-4ae2-9e01-9165ed979bab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def col_compare(src_col1 , tgt_col2):\n",
    "    \"\"\"Compares Two Columns and Flaghs 'U' if there is a CChange and Flags 'R' if there is no Change\"\"\"\n",
    "    if src_col1.lower() == tgt_col2.lower():\n",
    "        value = \"R\"\n",
    "    else:\n",
    "        value = \"U\"\n",
    "    return value\n",
    "\n",
    "cand_udf = F.udf(lambda x , y : col_compare(x , y))\n",
    "\n",
    "def erase_dim_from_col(df):\n",
    "    \"\"\"Erases 'dim_' From column names of each column in the input table\"\"\"\n",
    "    for value in df.columns:\n",
    "        df = df.withColumnRenamed(value , value[4:])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "390a290d-692b-45be-80af-643eb902425a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def olap_insert(df , df_dim):\n",
    "    \"\"\"'Left Joins' the Source and Target Tables on the basis of table_id and filters the \n",
    "    join on the basis of null record_keys for the corresponding table_id and \n",
    "    finally renames table_timestamp to table_oltp_timestamp\"\"\"\n",
    "    \n",
    "    insert = df. \\\n",
    "    join(df_dim , \n",
    "         F.col([value \n",
    "                for value \n",
    "                in df.columns \n",
    "                if value[-2:] == \"id\"][0]) == F.col([value \n",
    "                                                     for value \n",
    "                                                     in df_dim.columns \n",
    "                                                     if value[-2:] == \"id\"][0]) , \"left\"). \\\n",
    "    filter(F.isnull(F.col(f\"dim_{table}_record_key\"))). \\\n",
    "    select(df.columns). \\\n",
    "    withColumnRenamed(f\"{table}_timestamp\" , f\"{table}_oltp_timestamp\")\n",
    "    return df\n",
    "\n",
    "def olap_update(df , df_dim):\n",
    "    \"\"\"'Left Joins' the Source and Target Tables on the basis of table_id and \n",
    "    filters the join on the basis of NOT NULL record_keys \n",
    "    for the corresponding table_id in Target table and finally \n",
    "    creates a flag by comparing all the \n",
    "    non-key , non-flag and non-datetime columns with \n",
    "    R if no change and U if there are any changes\"\"\"\n",
    "\n",
    "    update_df = df. \\\n",
    "    join(df_dim , F.col([value \n",
    "                         for value \n",
    "                         in df.columns \n",
    "                         if value[-2:] == \"id\"][0]) == F.col([value \n",
    "                                                              for value \n",
    "                                                              in df_dim.columns \n",
    "                                                              if value[-2:] == \"id\"][0]) , \"left\"). \\\n",
    "    filter(~F.isnull(F.col(f\"dim_{table}_record_key\"))). \\\n",
    "    withColumn(\"update_stage_parameter\" , \n",
    "               F.concat_ws(\"_\" , *[value \n",
    "                                   for value \n",
    "                                   in df.columns \n",
    "                                   if value.split(\"_\")[-1] != \"timestamp\"])). \\\n",
    "    withColumn(\"update_dim_parameter\" , \n",
    "               F.concat_ws(\"_\" , \n",
    "                           *[value for value in df_dim.columns \n",
    "                             if value.lower().split(\"_\")[-1] != \"timestamp\" \n",
    "                             and value.lower().split(\"_\")[-1] != \"key\" \n",
    "                             and value.lower().split(\"_\")[-1] != \"flag\" \n",
    "                             and value.lower().split(\"_\")[-1] != \"date\"])). \\\n",
    "    withColumn(\"update_flag\" , cand_udf(F.col(\"update_stage_parameter\") , F.col(\"update_dim_parameter\")))\n",
    "    return update_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ad67d02-c198-49ff-ae0b-372c42f93115",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def scd_general_joiner(table , df , df_dim , scd_type = 2):\n",
    "    \"\"\"Joins the nessesary tables and gives the required output for the election project and \n",
    "    changes name of table_timestamp to table_oltp_timestamp to allign it with OLAP tables \n",
    "    for all input tables\"\"\"\n",
    "    global records_select\n",
    "    records_select = [value[4:] for value in df_dim.columns if value.lower().split(\"_\")[-1] != \"key\"]\n",
    "    if table == \"candidate\":\n",
    "        df_crime = spark.read.format(\"parquet\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"sep\", \",\") \\\n",
    "        .load(f\"/mnt/election-project-stage/candidates_has_crime_record_stage/as_on_dt={date_today}/candidates_has_crime_record_stage.parquet/\")\n",
    "        df_ed = spark.read.format(\"parquet\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"sep\", \",\") \\\n",
    "        .load(f\"/mnt/election-project-stage/candidate_education_level_stage/as_on_dt={date_today}/candidate_education_level_stage.parquet/\")\n",
    "\n",
    "        df = df_crime. \\\n",
    "        groupby(\"candidates_candidate_id\"). \\\n",
    "        agg(F.count(\"crime_record_crime_id\").\n",
    "            alias(\"candidate_no_of_crimes\")). \\\n",
    "        join(df , df_crime.candidates_candidate_id == df.candidate_id , \"inner\"). \\\n",
    "        join(df_ed , df_ed.education_level == df.education_level_candidate_education_level , \"inner\"). \\\n",
    "        select(\"candidate_id\" , \"candidate_name\" , \"candidate_spouse\" , \n",
    "               \"candidate_no_of_crimes\" , \"candidate_dob\" , \n",
    "               \"candidate_net_worth\" , \"highest_attained_education_level\" ,\n",
    "               \"candidates_timestamp\" , \"candidates_stage_timestamp\"). \\\n",
    "        withColumnRenamed(\"candidates_timestamp\" , \"candidate_oltp_timestamp\"). \\\n",
    "        withColumnRenamed(\"highest_attained_education_level\" , \"candidate_highest_education_level\")\n",
    "    elif table == \"constituency\":\n",
    "        df_risk = spark.read.format(\"parquet\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"sep\", \",\") \\\n",
    "        .load(f\"/mnt/election-project-stage/risk_stage/as_on_dt={date_today}/risk_stage.parquet/\")\n",
    "\n",
    "        df = df. \\\n",
    "        join(df_risk , F.col(\"risk_level\") == F.col(\"constituency_risk_level\") , \"inner\"). \\\n",
    "        select(\"constituency_id\" , \"constituency_name\" , \"district_of_constituency\" , \n",
    "               \"average_literacy_rate_in_constituency\" , \"risk_status\" , \n",
    "               \"constituency_timestamp\" , \"constituency_stage_timestamp\"). \\\n",
    "        withColumnRenamed(\"average_literacy_rate_in_constituency\" , \"average_constituency_literacy_rate\") .\\\n",
    "        withColumnRenamed(\"risk_status\" , \"constituency_risk\"). \\\n",
    "        withColumnRenamed(f\"{table}_timestamp\" , f\"{table}_oltp_timestamp\")\n",
    "    elif table == \"election_result\":\n",
    "        df_cand_const = spark.read.format(\"parquet\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"sep\", \",\") \\\n",
    "        .load(f\"/mnt/election-project-stage/candidates_has_constiuncy_stage/as_on_dt={date_today}/candidates_has_constiuncy_stage.parquet/\")\n",
    "\n",
    "        df_voters = spark.read.format(\"parquet\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"sep\", \",\") \\\n",
    "        .load(f\"/mnt/election-project-stage/voters_stage/as_on_dt={date_today}/voters_stage.parquet/\")\n",
    "\n",
    "        df_cand = spark.read.format(\"parquet\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"sep\", \",\") \\\n",
    "        .load(f\"/mnt/election-project-stage/candidates_stage/as_on_dt={date_today}/candidates_stage.parquet/\")\n",
    "\n",
    "        df = df_voters. \\\n",
    "        select(\"voter_id\" , \"polling_station_voter_polling_station_ID\"). \\\n",
    "        groupby(\"polling_station_voter_polling_station_ID\"). \\\n",
    "        agg(F.count(\"voter_id\").alias(\"polling_station_no_of_voters\")). \\\n",
    "        join(df , df.polling_station_polling_station_id == df_voters.polling_station_voter_polling_station_ID). \\\n",
    "        join(df_cand_const , df.candidate_candidate_id == df_cand_const.candidates_candidate_id). \\\n",
    "        join(df_cand , df_cand.candidate_id == df.candidate_candidate_id). \\\n",
    "        withColumnRenamed(\"election_id\" , \"polling_station_election_id\"). \\\n",
    "        withColumnRenamed(\"election_date\" , \"polling_station_election_date\"). \\\n",
    "        withColumnRenamed(\"vote_count\" , \"polling_station_vote_count\"). \\\n",
    "        select([value for value in df_dim.columns if value.lower().split(\"_\")[-1] != \"key\"])\n",
    "    elif table == \"polling_station\":\n",
    "        df = df.drop(\"constituency_constituency_id\").withColumnRenamed(f\"{table}_timestamp\" , f\"{table}_oltp_timestamp\")\n",
    "    else :\n",
    "        df = df. \\\n",
    "        withColumnRenamed(f\"{table}_timestamp\" , f\"{table}_oltp_timestamp\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82d47e2a-0e26-49cc-bc6a-6099804225ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def scd_type_one_frame_work(table , df , df_dim):\n",
    "    \"\"\"Works SCD type one for input Source and Target Tables\"\"\"\n",
    "    if table == \"election_result\":\n",
    "        df.createOrReplaceTempView(\"df_part_1\")\n",
    "        df_dim.createOrReplaceTempView(\"df_part_2\")\n",
    "\n",
    "        return spark.sql(f\"\"\"SELECT (\n",
    "        SELECT CASE \n",
    "        WHEN \n",
    "        ISNULL((SELECT MAX(election_result_key) AS max_ FROM df_part_2))\n",
    "        THEN 0\n",
    "        ELSE \n",
    "        (SELECT MAX(election_result_key) AS max_ FROM df_part_2)\n",
    "        END) + ROW_NUMBER() OVER(ORDER BY polling_station_election_id, \n",
    "        polling_station_polling_station_id , constituency_constituency_id , \n",
    "        candidates_candidate_id , candidates_candidate_id) \n",
    "        AS election_result_key, * \n",
    "        FROM df_part_1\"\"\")\n",
    "\n",
    "    else:\n",
    "        insert = olap_insert(df , df_dim)\n",
    "        insert = insert. \\\n",
    "        withColumn(f\"{table}_olap_timestamp\" , F.current_timestamp())\n",
    "\n",
    "        insert.createOrReplaceTempView(\"df_part_1\")\n",
    "\n",
    "        update_df = olap_update(df , df_dim)\n",
    "        update_df = update_df.filter(F.col(\"update_flag\") == \"U\"). \\\n",
    "                select(df.columns).withColumn(f\"{table}_olap_timestamp\" , F.current_timestamp())\n",
    "\n",
    "        update_df.createOrReplaceTempView(\"df_part_2\")\n",
    "\n",
    "        spark.sql(\n",
    "            \"\"\"SELECT * \n",
    "            FROM df_part_1\"\"\"). \\\n",
    "        unionByName(spark.sql(\n",
    "            \"\"\"SELECT * \n",
    "            FROM df_part_2\"\"\")).createOrReplaceTempView(\"df_part_1\")\n",
    "\n",
    "        final_op = spark.sql(f\"\"\"SELECT \n",
    "        ROW_NUMBER() OVER(ORDER BY {table}_id) AS {table}_record_key, * \n",
    "        FROM df_part_1\"\"\")\n",
    "    return final_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b68e294-c7ba-4692-8dfc-5f4fd0a76556",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def scd_type_two_frame_work(table , df , df_dim):\n",
    "    \"\"\"Works SCD type two for input Source and Target Tables\"\"\"\n",
    "    if table == \"election_result\":\n",
    "        df.createOrReplaceTempView(\"df_part_1\")\n",
    "        df_dim.createOrReplaceTempView(\"df_part_2\")\n",
    "\n",
    "        return spark.sql(f\"\"\"SELECT (\n",
    "        SELECT CASE \n",
    "        WHEN \n",
    "        ISNULL((SELECT MAX(election_result_key) AS max_ FROM df_part_2))\n",
    "        THEN 0\n",
    "        ELSE \n",
    "        (SELECT MAX(election_result_key) AS max_ FROM df_part_2)\n",
    "        END) + ROW_NUMBER() OVER(ORDER BY polling_station_election_id, polling_station_polling_station_id , constituency_constituency_id , candidates_candidate_id , candidates_candidate_id) AS election_result_key, * \n",
    "        FROM df_part_1\"\"\")\n",
    "\n",
    "    else:\n",
    "        alter_dob = [value for value in df_dim.columns if value.lower().split(\"_\")[-1] == \"dob\"]\n",
    "        if alter_dob != []:\n",
    "            for value in alter_dob:\n",
    "                df_dim = df_dim.withColumn(value , F.col(value).astype(T.DateType()))\n",
    "        \n",
    "        insert = df. \\\n",
    "    join(df_dim , \n",
    "         F.col([value \n",
    "                for value \n",
    "                in df.columns \n",
    "                if value[-2:] == \"id\"][0]) == F.col([value \n",
    "                                                     for value \n",
    "                                                     in df_dim.columns \n",
    "                                                     if value[-2:] == \"id\"][0]) , \"left\"). \\\n",
    "    filter(F.isnull(F.col(f\"dim_{table}_record_key\"))). \\\n",
    "    select(df.columns). \\\n",
    "    withColumnRenamed(f\"{table}_timestamp\" , f\"{table}_oltp_timestamp\")\n",
    "        update_insert = df. \\\n",
    "        join(df_dim , F.col([value \n",
    "                             for value \n",
    "                             in df.columns \n",
    "                             if value[-2:] == \"id\"][0]) == F.col([value \n",
    "                                                                  for value \n",
    "                                                                  in df_dim.columns \n",
    "                                                                  if value[-2:] == \"id\"][0]) , \"left\"). \\\n",
    "        filter(~F.isnull(F.col(f\"dim_{table}_record_key\"))). \\\n",
    "        select(df.columns). \\\n",
    "        withColumnRenamed(f\"{table}_timestamp\" , f\"{table}_oltp_timestamp\")\n",
    "\n",
    "        update_df = olap_update(df , df_dim)\n",
    "\n",
    "        update_update = update_df. \\\n",
    "        filter(F.col(\"update_flag\") == \"U\"). \\\n",
    "        select(df_dim.columns). \\\n",
    "        withColumn(\"dim_record_Active_Flag\" , F.lit(0)). \\\n",
    "        withColumn(f\"dim_{table}_record_end_date\" , F.current_timestamp())\n",
    "\n",
    "        op = update_df.filter(F.col(\"update_flag\") == \"R\").select(df_dim.columns)\n",
    "        \n",
    "        update_update = erase_dim_from_col(update_update)\n",
    "        op = erase_dim_from_col(op)\n",
    "\n",
    "        records_to_update = list(map(lambda x : x[0] , op.select(f\"{table}_id\").collect()))\n",
    "\n",
    "        df_part_1 = insert. \\\n",
    "        union(update_insert. \\\n",
    "              filter((~F.col(f\"{table}_id\").isin(records_to_update)))). \\\n",
    "        withColumn(\"record_Active_Flag\" , F.lit(1)). \\\n",
    "        withColumn(f\"{table}_record_start_date\" , F.current_timestamp()). \\\n",
    "        withColumn(f\"{table}_record_end_date\" , F.to_date(F.lit(\"9999-12-31 00:00:00\") , \"yyyy-MM-dd HH:mm:ss\")). \\\n",
    "        select(records_select)\n",
    "        df_part_1.createOrReplaceTempView(\"df_part_1\")\n",
    "\n",
    "        df_part_2 = op.unionByName(update_update)\n",
    "        df_part_2.createOrReplaceTempView(\"df_part_2\")\n",
    "\n",
    "        final_op = spark.sql(\"\"\"SELECT * FROM df_part_2\"\"\"). \\\n",
    "        unionByName(spark.sql(f\"\"\"SELECT (\n",
    "        SELECT CASE WHEN ISNULL((\n",
    "        SELECT MAX({table}_record_key) AS max_ FROM df_part_2)) \n",
    "        THEN 0 \n",
    "        ELSE (\n",
    "        SELECT MAX({table}_record_key) AS max_ \n",
    "        FROM df_part_2) END) + ROW_NUMBER() OVER(\n",
    "        ORDER BY {table}_id) AS {table}_record_key, * \n",
    "        FROM df_part_1\"\"\"))\n",
    "        return final_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "659ce71c-8282-4752-adeb-c3fb9f90da24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def scd_type_three_frame_work(table , df , df_dim):\n",
    "    \"\"\"Works SCD type three for input Source and Target Tables\"\"\"\n",
    "    if table == \"election_result\":\n",
    "        df.createOrReplaceTempView(\"df_part_1\")\n",
    "        df_dim.createOrReplaceTempView(\"df_part_2\")\n",
    "\n",
    "        return spark.sql(f\"\"\"SELECT (\n",
    "        SELECT CASE \n",
    "        WHEN \n",
    "        ISNULL((SELECT MAX(election_result_key) AS max_ FROM df_part_2))\n",
    "        THEN 0\n",
    "        ELSE \n",
    "        (SELECT MAX(election_result_key) AS max_ FROM df_part_2)\n",
    "        END) + ROW_NUMBER() OVER(ORDER BY \n",
    "        polling_station_election_id, polling_station_polling_station_id , \n",
    "        constituency_constituency_id , candidates_candidate_id , \n",
    "        candidates_candidate_id) \n",
    "        AS election_result_key, * FROM df_part_1\"\"\")\n",
    "\n",
    "    else:\n",
    "        update_df = olap_update(df , df_dim). \\\n",
    "                filter(F.col(\"update_flag\") == \"U\"). \\\n",
    "                withColumn(f\"{table}_oalp_old_timestamp\" , F.col(f\"dim_{table}_oalp_timestamp\")). \\\n",
    "                withColumn(f\"{table}_oalp_timestamp\" , F.current_timestamp()). \\\n",
    "               select([value[4:] \n",
    "                       for value in df_dim.columns \n",
    "                       if value.split(\"_\")[-1] != \"key\" \n",
    "                       and value.split(\"_\")[-1] != \"flag\"])\n",
    "\n",
    "\n",
    "        insert = olap_insert(df , df_dim). \\\n",
    "                withColumn(f\"{table}_oalp_timestamp\" , F.current_timestamp()). \\\n",
    "                withColumn(f\"{table}_oalp_old_timestamp\" , F.lit(None)). \\\n",
    "        where(~F.col(f\"{table}_id\").isin(list(map(lambda x : x[0] , update_df.select(f\"{table}_id\").collect()))))\n",
    "        \n",
    "        insert.createOrReplaceTempView(\"df_part_1\")\n",
    "        update_df.createOrReplaceTempView(\"df_part_2\")\n",
    "\n",
    "        spark.sql(\"\"\"SELECT * FROM df_part_1\"\"\"). \\\n",
    "        unionByName(\n",
    "            spark.sql(\"\"\"SELECT * FROM df_part_2\"\"\")) \\\n",
    "        .createOrReplaceTempView(\"df_part_1\")\n",
    "\n",
    "\n",
    "        erase_dim_from_col(df_dim).createOrReplaceTempView(\"df_part_2\")\n",
    "\n",
    "        final_op = spark.sql(f\"\"\"SELECT ROW_NUMBER() OVER(\n",
    "        ORDER BY {table}_id) AS {table}_record_key, * \n",
    "        FROM df_part_1\"\"\")\n",
    "    return final_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "153d16ce-4b2f-47f1-b8e1-3c2b761f7c7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def scd_over_write_to_hive(mode  , location , name , final_op):\n",
    "    if name.split(\"_\")[0].lower() == \"candidate\":\n",
    "        name = name + \"s\"\n",
    "    elif name == \"election_result_olap\":\n",
    "        name = \"election_result_fact\"\n",
    "    else:\n",
    "        name = name\n",
    "    list_of_columns_to_change_dtypes = [value[0] for value in final_op.dtypes if value[1] == \"long\" or value[1] == \"bigint\"]\n",
    "    if list_of_columns_to_change_dtypes != []:\n",
    "        for value in list_of_columns_to_change_dtypes:\n",
    "            final_op = final_op.withColumn(value , F.col(value).astype(T.IntegerType()))\n",
    "    \"\"\"Overwrites data in Target hive Tables with the processed Tables\"\"\"\n",
    "    final_op.write.format(mode).mode(\"overwrite\").save(location + \"/\" + name)\n",
    "    # \"delta\"   f\"/tmp/delta/metastore.election_olap.{table}_olap\"\n",
    "    return print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7612b53-a9eb-4c4b-a0f6-969f30438ecd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.widgets.combobox(name = \"star_selector\" , label=\"Choose the OLAP Table\" , defaultValue=\"\" , choices=[\"candidates\" , \"party\" ,\n",
    "#                                                                              \"constituency\" , \"polling_station\", \n",
    "#                                                                             \"election_result_fact\"])\n",
    "\n",
    "# dbutils.widgets.combobox(name = \"scd_selector\" , label=\"Choose the SCD Type\" , defaultValue=\"\" , choices=[\"1\",\"2\",\"3\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a035ab62-4c42-4b0a-b1b4-7e9c6c525079",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def scd_job_run(scd_type , table_name):\n",
    "    \"\"\"Runs all Nessesary Jobs based on Table Names and SCD Type as the input\"\"\"\n",
    "    if scd_type == 1:\n",
    "        try:\n",
    "            try:\n",
    "                table = get_data(table_name , scd_type)\n",
    "            except Exception as ex:\n",
    "                print(f\"Error has Occurred , {ex}\")\n",
    "                sys.exit()\n",
    "            try:\n",
    "                df1 = scd_general_joiner(table , df , df_dim , scd_type)\n",
    "            except Exception as ex:\n",
    "                print(f\"Error has Occurred , {ex}\")\n",
    "                sys.exit()\n",
    "            try:\n",
    "                op = scd_type_one_frame_work(table , df1 , df_dim)\n",
    "            except Exception as ex:\n",
    "                print(f\"Error has Occurred , {ex}\")\n",
    "                sys.exit()\n",
    "            return op\n",
    "        except:\n",
    "            return print(\"There was an Error while Executing\")\n",
    "    elif scd_type == 2:\n",
    "        try:\n",
    "            try:\n",
    "                table = get_data(table_name , scd_type)\n",
    "            except Exception as ex:\n",
    "                print(f\"Error has Occurred , {ex}\")\n",
    "                sys.exit()\n",
    "            try:\n",
    "                df1 = scd_general_joiner(table , df , df_dim , scd_type)\n",
    "            except Exception as ex:\n",
    "                print(f\"Error has Occurred , {ex}\")\n",
    "                sys.exit()\n",
    "            try:\n",
    "                op = scd_type_two_frame_work(table , df1 , df_dim)\n",
    "            except Exception as ex:\n",
    "                print(f\"Error has Occurred , {ex}\")\n",
    "                sys.exit()\n",
    "            return op\n",
    "        except:\n",
    "            return print(\"There was an Error while Executing\")\n",
    "    elif scd_type == 3:\n",
    "        try:\n",
    "            try:\n",
    "                table = get_data(table_name , scd_type)\n",
    "            except Exception as ex:\n",
    "                print(f\"Error has Occurred , {ex}\")\n",
    "                sys.exit()\n",
    "            try:\n",
    "                df1 = scd_general_joiner(table , df , df_dim , scd_type)\n",
    "            except Exception as ex:\n",
    "                print(f\"Error has Occurred , {ex}\")\n",
    "                sys.exit()\n",
    "            try:\n",
    "                op = scd_type_three_frame_work(table , df1 , df_dim)\n",
    "            except Exception as ex:\n",
    "                print(f\"Error has Occurred , {ex}\")\n",
    "                sys.exit()\n",
    "            return op\n",
    "        except:\n",
    "            return print(\"There was an Error while Executing\")\n",
    "    else:\n",
    "        print(\"Define the SCD Type\")\n",
    "        sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "335f7cc0-865c-4ff7-b52a-5bca16939a3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_job(list_ = [\"election_result_fact\" , \"candidates\" , \"party\" , \"constituency\" , \"polling_station\"]):\n",
    "    \"\"\"Runs jobs for all tables for given list of table names if data is already given in get data function\"\"\"\n",
    "    for value in list_:\n",
    "            output = scd_job_run(2 , value)\n",
    "            output.write.mode(\"overwrite\").parquet(f\"{op_MOUNT_NAME}/election-project-olap/{value}_olap/\")\n",
    "            print(f\"{value} Done\")\n",
    "    return print(\"All Tables Done ,  Check S3 for Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56d2bcd5-3038-4dae-a01f-1b85b955313e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polling_station Done\nAll Tables Done ,  Check S3 for Data\n"
     ]
    }
   ],
   "source": [
    "run_job()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1654984303046268,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "election_stage_to_OLAP",
   "notebookOrigID": 129523287884963,
   "widgets": {
    "scd_selector": {
     "currentValue": "2",
     "nuid": "141d9113-1cf1-4da1-aceb-2eb4d6094501",
     "widgetInfo": {
      "widgetType": "combobox",
      "defaultValue": "",
      "label": "Choose the SCD Type",
      "name": "scd_selector",
      "options": {
       "widgetType": "dropdown",
       "choices": [
        "1",
        "2",
        "3"
       ]
      }
     }
    },
    "star_selector": {
     "currentValue": "polling_station",
     "nuid": "e2a8f325-598c-4b18-8827-79d08e59e1e2",
     "widgetInfo": {
      "widgetType": "combobox",
      "defaultValue": "",
      "label": "Choose the OLAP Table",
      "name": "star_selector",
      "options": {
       "widgetType": "dropdown",
       "choices": [
        "candidates",
        "party",
        "constituency",
        "polling_station",
        "election_result_fact"
       ]
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
